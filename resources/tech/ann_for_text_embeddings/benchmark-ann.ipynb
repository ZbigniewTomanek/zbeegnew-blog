{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pkl\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import uuid\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from typing import Callable, Type\n",
    "from sklearn.cluster import KMeans\n",
    "from annoy import AnnoyIndex\n",
    "import os\n",
    "from time import time\n",
    "from tqdm import tqdm\n",
    "from hnswlib import Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"out/embeddings.pkl\", \"rb\") as f:\n",
    "    data = pkl.load(f)\n",
    "text_df, embeddings_df = data\n",
    "print(len(text_df), len(embeddings_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HyperBin:\n",
    "    def __init__(self, num_hyperplanes: int, num_dims: int) -> None:\n",
    "        self.uuid = str(uuid.uuid4())\n",
    "        self.hyperplanes = np.random.normal(size=(num_hyperplanes, num_dims))\n",
    "        norms = np.linalg.norm(self.hyperplanes, axis=1, keepdims=True)\n",
    "        self.hyperplanes = self.hyperplanes / norms\n",
    "\n",
    "    def get_bin(self, point: np.array) -> int:\n",
    "        dot_products = np.dot(self.hyperplanes, point)\n",
    "        bin_index = np.where(dot_products > 0, 1, 0)\n",
    "        bin_index = int(\"\".join(bin_index.astype(str)), 2)\n",
    "        return bin_index\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_arr = np.array([e for e in embeddings_df[\"embeddings\"]])\n",
    "print(embeddings_arr.shape)\n",
    "\n",
    "def test_bins(num_hyperplanes: int, num_dims: int) -> None:\n",
    "    h = HyperBin(num_hyperplanes, num_dims)\n",
    "    bins = []\n",
    "    for i in range(len(embeddings_arr)):\n",
    "        point = embeddings_arr[i]\n",
    "        bins.append(h.get_bin(point))\n",
    "    plt.hist(bins, bins=2**num_hyperplanes)\n",
    "    plt.show()\n",
    "    \n",
    "test_bins(8, 384)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_distance(vector1: np.array, vector2: np.array) -> float:\n",
    "    dot_product = np.dot(vector1, vector2)\n",
    "    norm_vector1 = np.linalg.norm(vector1)\n",
    "    norm_vector2 = np.linalg.norm(vector2)\n",
    "    cosine_similarity = dot_product / (norm_vector1 * norm_vector2)\n",
    "    cosine_distance = 1 - cosine_similarity\n",
    "    return cosine_distance\n",
    "\n",
    "def euclidean_distance(vector1: np.array, vector2: np.array) -> float:\n",
    "    return np.linalg.norm(vector1 - vector2)\n",
    "\n",
    "class LSHIndex:\n",
    "    def __init__(self, num_hyperplanes: int, num_dims: int, num_bins: int, distance_func: Callable[[np.array, np.array], float], **kwargs) -> None:\n",
    "        self._all_points = []\n",
    "        self.dinstance_func = distance_func\n",
    "        self._hyperbins = [HyperBin(num_hyperplanes, num_dims) for _ in range(num_bins)]\n",
    "        self._index: dict[str, dict[int, list[np.array]]] = {bin.uuid: {} for bin in self._hyperbins}\n",
    "\n",
    "    def index(self, point: np.array) -> None:\n",
    "        self._all_points.append(point)\n",
    "        for bin in self._hyperbins:\n",
    "            bin_index = bin.get_bin(point)\n",
    "            if bin_index not in self._index[bin.uuid]:\n",
    "                self._index[bin.uuid][bin_index] = []\n",
    "            self._index[bin.uuid][bin_index].append(point)\n",
    "\n",
    "    def search(self, point: np.array, use_index: bool, limit: int) -> list[tuple[float, str]]:\n",
    "        candidates = []\n",
    "        if use_index:\n",
    "            c_hashes = set()\n",
    "            for bin in self._hyperbins:\n",
    "                bin_index = bin.get_bin(point)\n",
    "                if bin_index in self._index[bin.uuid]:\n",
    "                    for candidate in self._index[bin.uuid][bin_index]:\n",
    "                        c_hash = hash(candidate.tobytes())\n",
    "                        if c_hash not in c_hashes:\n",
    "                            candidates.append(candidate)\n",
    "                            c_hashes.add(c_hash)\n",
    "        else:\n",
    "            candidates = self._all_points\n",
    "\n",
    "        distances = []\n",
    "        for candidate in candidates:\n",
    "            distance = self.dinstance_func(candidate, point)\n",
    "            distances.append((distance, candidate))\n",
    "            \n",
    "        distances.sort(key=lambda x: x[0])\n",
    "        return [{\"distance\": d, \"point_hash\": hash(a.tobytes())} for d, a in distances[:limit]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = LSHIndex(8, 384, 1, distance_func=cosine_distance)\n",
    "for embedding in embeddings_arr:\n",
    "    index.index(embedding)\n",
    "\n",
    "bin_sizes = []\n",
    "for uid, bin in index._index.items():\n",
    "    for bin_id, points in bin.items():\n",
    "        bin_sizes.append(len(points))\n",
    "\n",
    "print(f\"Average bin size: {np.mean(bin_sizes)}\")\n",
    "print(f\"Max bin size: {np.max(bin_sizes)}\")\n",
    "print(f\"Min bin size: {np.min(bin_sizes)}\")\n",
    "index.search(embeddings_arr[0], use_index=True, limit=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark(\n",
    "        embeddings: list[np.array],\n",
    "        distance_fun: Callable[[np.array, np.array], float],\n",
    "        index_cls: Type[LSHIndex],\n",
    "        search_limit: int,\n",
    "        test_sample_size: int,\n",
    ") -> None:\n",
    "    def count_misses(num_of_bins: int, limit: int, num_of_hyperplanes: int, distance_fun) -> int:\n",
    "        index = index_cls(num_hyperplanes=num_of_hyperplanes, num_dims=384, num_bins=num_of_bins, distance_func=distance_fun, embeddings=embeddings)\n",
    "        for embedding in embeddings[:-test_sample_size]:\n",
    "            index.index(embedding)\n",
    "        misses = 0\n",
    "        for random_embedding in embeddings[-test_sample_size:]:\n",
    "            with_index = index.search(random_embedding, use_index=True, limit=limit)\n",
    "            no_index = index.search(random_embedding, use_index=False, limit=limit)\n",
    "            diff = len(set([x[\"point_hash\"] for x in with_index]) - set([x[\"point_hash\"] for x in no_index]))\n",
    "            misses += diff\n",
    "        return misses\n",
    "\n",
    "    data = []\n",
    "    for num_of_bins in range(1, 11):\n",
    "        for num_of_hyperplanes in range(4, 10):\n",
    "            misses = count_misses(num_of_bins, limit=search_limit, num_of_hyperplanes=num_of_hyperplanes, distance_fun=cosine_distance)\n",
    "            data.append((num_of_bins, num_of_hyperplanes, misses))\n",
    "            print(f\"num_of_bins: {num_of_bins}, num_of_hyperplanes: {num_of_hyperplanes}, missed: {misses}/{test_sample_size * search_limit}\")\n",
    "\n",
    "    df = pd.DataFrame(data, columns=[\"num_of_bins\", \"num_of_hyperplanes\", \"misses\"])\n",
    "    df = df.pivot(index=\"num_of_bins\", columns=\"num_of_hyperplanes\", values=\"misses\")\n",
    "\n",
    "    title=f\"{index_cls.__name__} {distance_fun.__name__} misses\"\n",
    "    ax = plt.axes()\n",
    "    sns.heatmap(df, annot=True, fmt=\"d\", ax=ax)\n",
    "    ax.set_title(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_arr_sample = np.array([np.array(e) for e in embeddings_arr[:10_000]])\n",
    "print(embeddings_arr_sample.shape)\n",
    "test_sample_size = 100\n",
    "search_limit = 30\n",
    "distance_fun = cosine_distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmark(\n",
    "    embeddings=embeddings_arr_sample,\n",
    "    distance_fun=cosine_distance,\n",
    "    index_cls=LSHIndex,\n",
    "    search_limit=search_limit,\n",
    "    test_sample_size=test_sample_size,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CenterdHyperBin(HyperBin):\n",
    "    def __init__(self, num_hyperplanes: int, center: np.array) -> None:\n",
    "        self.uuid = str(uuid.uuid4())\n",
    "        num_dims = len(center)\n",
    "        self.hyperplanes = np.random.normal(size=(num_hyperplanes, num_dims))\n",
    "        self.hyperplanes = self.hyperplanes - center  # shift hyperplanes to cross on the center of mass\n",
    "\n",
    "        norms = np.linalg.norm(self.hyperplanes, axis=1, keepdims=True)\n",
    "        self.hyperplanes = self.hyperplanes / norms\n",
    "\n",
    "class CenteredLSHIndex(LSHIndex):\n",
    "    def __init__(self, num_hyperplanes: int, num_bins: int, distance_func: Callable[[np.array, np.array], float], embeddings: list[np.array], **kwargs) -> None:\n",
    "        self._all_points = []\n",
    "        self.dinstance_func = distance_func\n",
    "        center = np.mean(embeddings, axis=0)\n",
    "        self._hyperbins = [CenterdHyperBin(num_hyperplanes, center) for _ in range(num_bins)]\n",
    "        self._index: dict[str, dict[int, list[np.array]]] = {bin.uuid: {} for bin in self._hyperbins}\n",
    "        self._hash_cache = {}\n",
    "\n",
    "        for point in self._all_points:\n",
    "            self.index(point)\n",
    "\n",
    "benchmark(\n",
    "    embeddings=embeddings_arr_sample,\n",
    "    distance_fun=cosine_distance,\n",
    "    index_cls=CenteredLSHIndex,\n",
    "    search_limit=search_limit,\n",
    "    test_sample_size=test_sample_size,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KMeansHyperBin(HyperBin):\n",
    "    def __init__(self, hyperplanes: np.array) -> None:\n",
    "        self.uuid = str(uuid.uuid4())\n",
    "        self.hyperplanes = hyperplanes\n",
    "        norms = np.linalg.norm(self.hyperplanes, axis=1, keepdims=True)\n",
    "        self.hyperplanes = self.hyperplanes / norms\n",
    "\n",
    "class KMeansLSHIndex(LSHIndex):\n",
    "    def __init__(self, num_hyperplanes: int, num_bins: int, distance_func: Callable[[np.array, np.array], float], embeddings: list[np.array], **kwargs) -> None:\n",
    "        self._all_points = []\n",
    "        self.dinstance_func = distance_func\n",
    "\n",
    "        # Use kmeans to initialize hyperplanes\n",
    "        kmeans = KMeans(n_clusters=num_hyperplanes, n_init=\"auto\").fit(embeddings)\n",
    "        hyperplanes = kmeans.cluster_centers_\n",
    "\n",
    "        self._hyperbins = [KMeansHyperBin(hyperplanes) for _ in range(num_bins)]\n",
    "        self._index: dict[str, dict[int, list[np.array]]] = {bin.uuid: {} for bin in self._hyperbins}\n",
    "        self._hash_cache = {}\n",
    "\n",
    "        for point in self._all_points:\n",
    "            self.index(point)\n",
    "\n",
    "benchmark(\n",
    "    embeddings=embeddings_arr_sample,\n",
    "    distance_fun=cosine_distance,\n",
    "    index_cls=KMeansLSHIndex,\n",
    "    search_limit=search_limit,\n",
    "    test_sample_size=test_sample_size,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "for i in range(4, 15):\n",
    "    number_of_hyperplanes = i\n",
    "    start_time = time()\n",
    "    kmeans_index = KMeansLSHIndex(num_hyperplanes=9, num_bins=1, distance_func=cosine_distance, embeddings=embeddings_arr)\n",
    "    end_time = time()\n",
    "    init_time = end_time - start_time\n",
    "    print(f\"number_of_hyperplanes: {number_of_hyperplanes}, init_time: {init_time}\")\n",
    "    data.append((number_of_hyperplanes, init_time))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data, columns=[\"number_of_hyperplanes\", \"init_time_s\"])\n",
    "ax = plt.axes()\n",
    "\n",
    "sns.pointplot(data=df, x=\"number_of_hyperplanes\", y=\"init_time_s\", ax=ax, linestyles=\"--\")\n",
    "ax.set_title(\"KMeansLSHIndex init time s by number of hyperplanes\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annoy_metrics = [\"angular\", \"euclidean\", \"manhattan\", \"hamming\", \"dot\"]\n",
    "\n",
    "def annoy_count_misses(\n",
    "    embeddings: list[np.array],\n",
    "    distance_fun: Callable[[np.array, np.array], float],\n",
    "    annoy_metric: str,\n",
    "    search_limit: int,\n",
    "    test_sample_size: int,\n",
    "    num_trees: int,\n",
    ") -> int:\n",
    "\n",
    "    dimension = embeddings[0].shape[0]\n",
    "    index = AnnoyIndex(dimension, metric=annoy_metric)\n",
    "    \n",
    "    # Indexing\n",
    "    for i, embedding in enumerate(embeddings[:-test_sample_size]):\n",
    "        index.add_item(i, embedding)\n",
    "    index.build(num_trees)\n",
    "\n",
    "    misses = 0\n",
    "    for i, random_embedding in enumerate(embeddings[-test_sample_size:]):\n",
    "        with_index = index.get_nns_by_vector(random_embedding, search_limit)\n",
    "        \n",
    "        # Compute distances to all points and get the closest ones\n",
    "        distances = [(j, distance_fun(random_embedding, embeddings[j])) for j in range(len(embeddings))]\n",
    "        distances.sort(key=lambda x: x[1])\n",
    "        no_index = [j for j, _ in distances[:search_limit]]\n",
    "\n",
    "        diff = len(set(with_index) - set(no_index))\n",
    "        misses += diff\n",
    "\n",
    "    return misses\n",
    "\n",
    "def benchmark_annoy(\n",
    "    embeddings: list[np.array],\n",
    "    distance_fun: Callable[[np.array, np.array], float],\n",
    "    search_limit: int,\n",
    "    test_sample_size: int,\n",
    ") -> None:\n",
    "    \n",
    "    data = []\n",
    "    for num_trees in range(10, 100, 10):  \n",
    "        for metric in annoy_metrics:\n",
    "            misses = annoy_count_misses(embeddings, distance_fun, metric, search_limit, test_sample_size, num_trees)\n",
    "            data.append((num_trees, misses, metric))\n",
    "            print(f\"num_trees: {num_trees}, metric: {metric}, missed: {misses}/{test_sample_size * search_limit}\")\n",
    "\n",
    "    df = pd.DataFrame(data, columns=[\"num_trees\", \"misses\", \"metric\"])\n",
    "    df = df.pivot(index=\"num_trees\", columns=\"metric\", values=\"misses\")\n",
    "    \n",
    "    title = f\"Annoy vs {distance_fun.__name__} misses\"\n",
    "    ax = plt.axes()\n",
    "    sns.heatmap(df, annot=True, fmt=\"d\", ax=ax)\n",
    "    ax.set_title(title)\n",
    "\n",
    "benchmark_annoy(\n",
    "    embeddings=embeddings_arr_sample,\n",
    "    distance_fun=cosine_distance,\n",
    "    search_limit=search_limit,\n",
    "    test_sample_size=test_sample_size,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_annoy_index(embeddings: list[np.array], trees: int) -> dict[str, float]:\n",
    "    os.makedirs(\"out\", exist_ok=True)\n",
    "    file_name = \"out/index.ann\"\n",
    "\n",
    "    start = time()\n",
    "    index = AnnoyIndex(384, metric=\"angular\")\n",
    "    for i, embedding in enumerate(embeddings):\n",
    "        index.add_item(i, embedding)\n",
    "    index.build(trees)\n",
    "    indexing_time = time() - start\n",
    "\n",
    "    start = time()\n",
    "    index.save(file_name)\n",
    "    end = time()\n",
    "    index_save_time = end - start\n",
    "\n",
    "    start = time()\n",
    "    index = AnnoyIndex(384, metric=\"angular\")\n",
    "    index.load(file_name)\n",
    "    end = time()\n",
    "    index_load_time = end - start\n",
    "\n",
    "    return {\n",
    "        'indexing_time': indexing_time,\n",
    "        'save_time': index_save_time,\n",
    "        'load_time': index_load_time,\n",
    "        'index_size': os.path.getsize(file_name) / 1024 / 1024\n",
    "    }\n",
    "\n",
    "def benchmark_annoy_index_and_plot(embeddings_ar, embeddings_step, trees_range, trees_step) -> None:\n",
    "    results = pd.DataFrame(columns=['trees', 'embeddings', 'indexing_time', 'save_time', 'load_time', 'index_size'])\n",
    "\n",
    "    for i in tqdm(range(embeddings_step, len(embeddings_ar), embeddings_step)):\n",
    "        embeddings = embeddings_ar[:i] \n",
    "        for trees in range(trees_step, trees_range, trees_step):\n",
    "            stats = benchmark_annoy_index(embeddings, trees)\n",
    "            data = {\n",
    "                'trees': trees,\n",
    "                'embeddings': len(embeddings),\n",
    "                'indexing_time': stats['indexing_time'],\n",
    "                'save_time': stats['save_time'],\n",
    "                'load_time': stats['load_time'],\n",
    "                'index_size': stats['index_size']\n",
    "            }\n",
    "            print(data)\n",
    "            results = pd.concat([results, pd.DataFrame(data, index=[0])])\n",
    "    \n",
    "    fig, axs = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    indexing_time = results.pivot('trees', 'embeddings', 'indexing_time')\n",
    "    save_time = results.pivot('trees', 'embeddings', 'save_time')\n",
    "    load_time = results.pivot('trees', 'embeddings', 'load_time')\n",
    "    index_size = results.pivot('trees', 'embeddings', 'index_size')\n",
    "\n",
    "    sns.heatmap(indexing_time, ax=axs[0, 0], cmap='YlGnBu', annot=True, fmt=\".5f\")\n",
    "    axs[0, 0].set_title('Indexing Time s')\n",
    "\n",
    "    sns.heatmap(save_time, ax=axs[0, 1], cmap='YlGnBu', annot=True, fmt=\".5f\")\n",
    "    axs[0, 1].set_title('Save Time s')\n",
    "\n",
    "    sns.heatmap(load_time, ax=axs[1, 0], cmap='YlGnBu', annot=True, fmt=\".5f\")\n",
    "    axs[1, 0].set_title('Load Time s')\n",
    "\n",
    "    sns.heatmap(index_size, ax=axs[1, 1], cmap='YlGnBu', annot=True, fmt=\".2f\")\n",
    "    axs[1, 1].set_title('Index Size MB')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "benchmark_annoy_index_and_plot(embeddings_arr, 25_000, 60, 10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hnsw_metrics = [\"l2\", \"ip\", \"cosine\"]\n",
    "\n",
    "def hnsw_count_misses(\n",
    "    embeddings: list[np.array],\n",
    "    distance_fun: Callable[[np.array, np.array], float],\n",
    "    hnsw_metric: str,\n",
    "    search_limit: int,\n",
    "    test_sample_size: int,\n",
    "    num_links: int,\n",
    ") -> int:\n",
    "\n",
    "    dimension = embeddings[0].shape[0]\n",
    "    index = Index(space=hnsw_metric, dim=dimension)\n",
    "    \n",
    "    # Indexing\n",
    "    index.init_index(max_elements=len(embeddings), ef_construction=200, M=num_links)\n",
    "    index.add_items(np.array(embeddings[:-test_sample_size]))\n",
    "\n",
    "    misses = 0\n",
    "    for i, random_embedding in enumerate(embeddings[-test_sample_size:]):\n",
    "        with_index = index.knn_query(random_embedding, k=search_limit)[0][0]\n",
    "\n",
    "        # Compute distances to all points and get the closest ones\n",
    "        distances = [(j, distance_fun(random_embedding, embeddings[j])) for j in range(len(embeddings))]\n",
    "        distances.sort(key=lambda x: x[1])\n",
    "        no_index = [j for j, _ in distances[:search_limit]]\n",
    "\n",
    "        diff = len(set(with_index) - set(no_index))\n",
    "        misses += diff\n",
    "\n",
    "    return misses\n",
    "\n",
    "def benchmark_hnsw(\n",
    "    embeddings: list[np.array],\n",
    "    distance_fun: Callable[[np.array, np.array], float],\n",
    "    search_limit: int,\n",
    "    test_sample_size: int,\n",
    ") -> None:\n",
    "    \n",
    "    data = []\n",
    "    for num_links in range(10, 100, 10):  \n",
    "        for metric in hnsw_metrics:\n",
    "            misses = hnsw_count_misses(embeddings, distance_fun, metric, search_limit, test_sample_size, num_links)\n",
    "            data.append((num_links, misses, metric))\n",
    "            print(f\"num_links: {num_links}, metric: {metric}, missed: {misses}/{test_sample_size * search_limit}\")\n",
    "\n",
    "    df = pd.DataFrame(data, columns=[\"num_links\", \"misses\", \"metric\"])\n",
    "    df = df.pivot(index=\"num_links\", columns=\"metric\", values=\"misses\")\n",
    "    \n",
    "    title = f\"HNSW vs {distance_fun.__name__} misses\"\n",
    "    ax = plt.axes()\n",
    "    sns.heatmap(df, annot=True, fmt=\"d\", ax=ax)\n",
    "    ax.set_title(title)\n",
    "\n",
    "benchmark_hnsw(\n",
    "    embeddings=embeddings_arr_sample,\n",
    "    distance_fun=cosine_distance,\n",
    "    search_limit=search_limit,\n",
    "    test_sample_size=test_sample_size,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def benchmark_hnsw_index(embeddings: list[np.array], links: int) -> dict[str, float]:\n",
    "    os.makedirs(\"out\", exist_ok=True)\n",
    "    file_name = \"out/index.hnsw\"\n",
    "\n",
    "    start = time()\n",
    "    index = Index(space=\"l2\", dim=embeddings[0].shape[0])\n",
    "    index.init_index(max_elements=len(embeddings), ef_construction=200, M=links)\n",
    "    index.add_items(embeddings)\n",
    "    indexing_time = time() - start\n",
    "\n",
    "    start = time()\n",
    "    index.save_index(file_name)\n",
    "    end = time()\n",
    "    index_save_time = end - start\n",
    "\n",
    "    start = time()\n",
    "    index = Index(space=\"l2\", dim=embeddings[0].shape[0])\n",
    "    index.load_index(file_name)\n",
    "    end = time()\n",
    "    index_load_time = end - start\n",
    "\n",
    "    return {\n",
    "        'indexing_time': indexing_time,\n",
    "        'save_time': index_save_time,\n",
    "        'load_time': index_load_time,\n",
    "        'index_size': os.path.getsize(file_name) / 1024 / 1024\n",
    "    }\n",
    "\n",
    "def benchmark_hnsw_index_and_plot(embeddings_ar, embeddings_step, links_range, links_step) -> None:\n",
    "    results = pd.DataFrame(columns=['links', 'embeddings', 'indexing_time', 'save_time', 'load_time', 'index_size'])\n",
    "\n",
    "    for i in tqdm(range(embeddings_step, len(embeddings_ar), embeddings_step)):\n",
    "        embeddings = embeddings_ar[:i] \n",
    "        for links in range(links_step, links_range, links_step):\n",
    "            stats = benchmark_hnsw_index(embeddings, links)\n",
    "            data = {\n",
    "                'links': links,\n",
    "                'embeddings': len(embeddings),\n",
    "                'indexing_time': stats['indexing_time'],\n",
    "                'save_time': stats['save_time'],\n",
    "                'load_time': stats['load_time'],\n",
    "                'index_size': stats['index_size']\n",
    "            }\n",
    "            print(data)\n",
    "            results = pd.concat([results, pd.DataFrame(data, index=[0])])\n",
    "    \n",
    "    fig, axs = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    indexing_time = results.pivot('links', 'embeddings', 'indexing_time')\n",
    "    save_time = results.pivot('links', 'embeddings', 'save_time')\n",
    "    load_time = results.pivot('links', 'embeddings', 'load_time')\n",
    "    index_size = results.pivot('links', 'embeddings', 'index_size')\n",
    "\n",
    "    sns.heatmap(indexing_time, ax=axs[0, 0], cmap='YlGnBu', annot=True, fmt=\".5f\")\n",
    "    axs[0, 0].set_title('Indexing Time s')\n",
    "\n",
    "    sns.heatmap(save_time, ax=axs[0, 1], cmap='YlGnBu', annot=True, fmt=\".5f\")\n",
    "    axs[0, 1].set_title('Save Time s')\n",
    "\n",
    "    sns.heatmap(load_time, ax=axs[1, 0], cmap='YlGnBu', annot=True, fmt=\".5f\")\n",
    "    axs[1, 0].set_title('Load Time s')\n",
    "\n",
    "    sns.heatmap(index_size, ax=axs[1, 1], cmap='YlGnBu', annot=True, fmt=\".2f\")\n",
    "    axs[1, 1].set_title('Index Size MB')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "benchmark_hnsw_index_and_plot(embeddings_arr, 25_000, 60, 10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
